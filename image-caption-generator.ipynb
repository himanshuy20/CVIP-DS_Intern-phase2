{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # Image Captioning ","metadata":{}},{"cell_type":"markdown","source":"# Import Modules","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:14:59.322785Z","iopub.execute_input":"2023-07-30T15:14:59.323502Z","iopub.status.idle":"2023-07-30T15:15:06.068106Z","shell.execute_reply.started":"2023-07-30T15:14:59.323386Z","shell.execute_reply":"2023-07-30T15:15:06.067049Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import Data_DIR , Work_DIR","metadata":{}},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/flickr8k'\nWORKING_DIR = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:15:06.070324Z","iopub.execute_input":"2023-07-30T15:15:06.071041Z","iopub.status.idle":"2023-07-30T15:15:06.077977Z","shell.execute_reply.started":"2023-07-30T15:15:06.070981Z","shell.execute_reply":"2023-07-30T15:15:06.076854Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Extract Image Features","metadata":{}},{"cell_type":"code","source":"# load vgg16 model\nmodel = VGG16()\n# restructure the model\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n# summarize\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:15:06.079727Z","iopub.execute_input":"2023-07-30T15:15:06.080407Z","iopub.status.idle":"2023-07-30T15:15:13.927765Z","shell.execute_reply.started":"2023-07-30T15:15:06.080362Z","shell.execute_reply":"2023-07-30T15:15:13.926608Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467904/553467096 [==============================] - 3s 0us/step\n553476096/553467096 [==============================] - 3s 0us/step\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n=================================================================\nTotal params: 134,260,544\nTrainable params: 134,260,544\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# extract features from image","metadata":{}},{"cell_type":"code","source":"features = {}\ndirectory = os.path.join(BASE_DIR, 'Images')\n\nfor img_name in tqdm(os.listdir(directory)):\n    # load the image from file\n    img_path = directory + '/' + img_name\n    image = load_img(img_path, target_size=(224, 224))\n    # convert image pixels to numpy array\n    image = img_to_array(image)\n    # reshape data for model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # preprocess image for vgg\n    image = preprocess_input(image)\n    # extract features\n    feature = model.predict(image, verbose=0)\n    # get image ID\n    image_id = img_name.split('.')[0]\n    # store feature\n    features[image_id] = feature","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:15:13.930523Z","iopub.execute_input":"2023-07-30T15:15:13.931211Z","iopub.status.idle":"2023-07-30T15:24:45.609173Z","shell.execute_reply.started":"2023-07-30T15:15:13.931172Z","shell.execute_reply":"2023-07-30T15:24:45.608134Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72764b6e75f34761a6a913f077216462"}},"metadata":{}}]},{"cell_type":"markdown","source":"# store features in pickle","metadata":{}},{"cell_type":"code","source":"# store features in pickle\npickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:45.610955Z","iopub.execute_input":"2023-07-30T15:24:45.611666Z","iopub.status.idle":"2023-07-30T15:24:45.964910Z","shell.execute_reply.started":"2023-07-30T15:24:45.611623Z","shell.execute_reply":"2023-07-30T15:24:45.963884Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load features from pickle\nwith open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:45.966187Z","iopub.execute_input":"2023-07-30T15:24:45.966559Z","iopub.status.idle":"2023-07-30T15:24:46.141852Z","shell.execute_reply.started":"2023-07-30T15:24:45.966523Z","shell.execute_reply":"2023-07-30T15:24:46.140848Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load the Captions Data","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n    next(f)\n    captions_doc = f.read()","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.144380Z","iopub.execute_input":"2023-07-30T15:24:46.144692Z","iopub.status.idle":"2023-07-30T15:24:46.220752Z","shell.execute_reply.started":"2023-07-30T15:24:46.144657Z","shell.execute_reply":"2023-07-30T15:24:46.219758Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# create mapping of image to captions\nmapping = {}\n# process lines\nfor line in tqdm(captions_doc.split('\\n')):\n    # split the line by comma(,)\n    tokens = line.split(',')\n    if len(line) < 2:\n        continue\n    image_id, caption = tokens[0], tokens[1:]\n    # remove extension from image ID\n    image_id = image_id.split('.')[0]\n    # convert caption list to string\n    caption = \" \".join(caption)\n    # create list if needed\n    if image_id not in mapping:\n        mapping[image_id] = []\n    # store the caption\n    mapping[image_id].append(caption)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.222399Z","iopub.execute_input":"2023-07-30T15:24:46.222796Z","iopub.status.idle":"2023-07-30T15:24:46.395142Z","shell.execute_reply.started":"2023-07-30T15:24:46.222758Z","shell.execute_reply":"2023-07-30T15:24:46.393974Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40456 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d74b56dcd1f4c78927d40d2d0de12d4"}},"metadata":{}}]},{"cell_type":"code","source":"len(mapping)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.396515Z","iopub.execute_input":"2023-07-30T15:24:46.397171Z","iopub.status.idle":"2023-07-30T15:24:46.407363Z","shell.execute_reply.started":"2023-07-30T15:24:46.397131Z","shell.execute_reply":"2023-07-30T15:24:46.405985Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"8091"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess Text Data","metadata":{}},{"cell_type":"code","source":"def clean(mapping):\n    for key, captions in mapping.items():\n        for i in range(len(captions)):\n            # take one caption at a time\n            caption = captions[i]\n            # preprocessing steps\n            # convert to lowercase\n            caption = caption.lower()\n            # delete digits, special chars, etc., \n            caption = caption.replace('[^A-Za-z]', '')\n            # delete additional spaces\n            caption = caption.replace('\\s+', ' ')\n            # add start and end tags to the caption\n            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n            captions[i] = caption","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.414003Z","iopub.execute_input":"2023-07-30T15:24:46.414421Z","iopub.status.idle":"2023-07-30T15:24:46.422505Z","shell.execute_reply.started":"2023-07-30T15:24:46.414381Z","shell.execute_reply":"2023-07-30T15:24:46.421373Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# before preprocess of text\nmapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.424366Z","iopub.execute_input":"2023-07-30T15:24:46.424901Z","iopub.status.idle":"2023-07-30T15:24:46.434583Z","shell.execute_reply.started":"2023-07-30T15:24:46.424860Z","shell.execute_reply":"2023-07-30T15:24:46.433437Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['A child in a pink dress is climbing up a set of stairs in an entry way .',\n 'A girl going into a wooden building .',\n 'A little girl climbing into a wooden playhouse .',\n 'A little girl climbing the stairs to her playhouse .',\n 'A little girl in a pink dress going into a wooden cabin .']"},"metadata":{}}]},{"cell_type":"code","source":"# preprocess the text\nclean(mapping)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.436185Z","iopub.execute_input":"2023-07-30T15:24:46.437320Z","iopub.status.idle":"2023-07-30T15:24:46.610607Z","shell.execute_reply.started":"2023-07-30T15:24:46.437235Z","shell.execute_reply":"2023-07-30T15:24:46.609576Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# after preprocess of text\nmapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.612301Z","iopub.execute_input":"2023-07-30T15:24:46.612685Z","iopub.status.idle":"2023-07-30T15:24:46.619647Z","shell.execute_reply.started":"2023-07-30T15:24:46.612634Z","shell.execute_reply":"2023-07-30T15:24:46.618537Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq']"},"metadata":{}}]},{"cell_type":"code","source":"all_captions = []\nfor key in mapping:\n    for caption in mapping[key]:\n        all_captions.append(caption)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.621699Z","iopub.execute_input":"2023-07-30T15:24:46.622568Z","iopub.status.idle":"2023-07-30T15:24:46.640358Z","shell.execute_reply.started":"2023-07-30T15:24:46.622452Z","shell.execute_reply":"2023-07-30T15:24:46.639349Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"len(all_captions)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.642310Z","iopub.execute_input":"2023-07-30T15:24:46.642846Z","iopub.status.idle":"2023-07-30T15:24:46.653237Z","shell.execute_reply.started":"2023-07-30T15:24:46.642807Z","shell.execute_reply":"2023-07-30T15:24:46.652066Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"40455"},"metadata":{}}]},{"cell_type":"code","source":"all_captions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.654931Z","iopub.execute_input":"2023-07-30T15:24:46.655603Z","iopub.status.idle":"2023-07-30T15:24:46.664177Z","shell.execute_reply.started":"2023-07-30T15:24:46.655560Z","shell.execute_reply":"2023-07-30T15:24:46.662914Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']"},"metadata":{}}]},{"cell_type":"code","source":"# tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:46.666281Z","iopub.execute_input":"2023-07-30T15:24:46.667223Z","iopub.status.idle":"2023-07-30T15:24:47.420018Z","shell.execute_reply.started":"2023-07-30T15:24:46.667176Z","shell.execute_reply":"2023-07-30T15:24:47.418940Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:47.421409Z","iopub.execute_input":"2023-07-30T15:24:47.422484Z","iopub.status.idle":"2023-07-30T15:24:47.430227Z","shell.execute_reply.started":"2023-07-30T15:24:47.422443Z","shell.execute_reply":"2023-07-30T15:24:47.429066Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"8485"},"metadata":{}}]},{"cell_type":"code","source":"# get maximum length of the caption available\nmax_length = max(len(caption.split()) for caption in all_captions)\nmax_length","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:47.432325Z","iopub.execute_input":"2023-07-30T15:24:47.433160Z","iopub.status.idle":"2023-07-30T15:24:47.474854Z","shell.execute_reply.started":"2023-07-30T15:24:47.433116Z","shell.execute_reply":"2023-07-30T15:24:47.473931Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"35"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"image_ids = list(mapping.keys())\nsplit = int(len(image_ids) * 0.90)\ntrain = image_ids[:split]\ntest = image_ids[split:]","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:47.476382Z","iopub.execute_input":"2023-07-30T15:24:47.476729Z","iopub.status.idle":"2023-07-30T15:24:47.483258Z","shell.execute_reply.started":"2023-07-30T15:24:47.476693Z","shell.execute_reply":"2023-07-30T15:24:47.482056Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# startseq girl going into wooden building endseq\n#        X                   y\n# startseq                   girl\n# startseq girl              going\n# startseq girl going        into\n# ...........\n# startseq girl going into wooden building      endseq","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:47.485409Z","iopub.execute_input":"2023-07-30T15:24:47.486455Z","iopub.status.idle":"2023-07-30T15:24:47.491874Z","shell.execute_reply.started":"2023-07-30T15:24:47.486413Z","shell.execute_reply":"2023-07-30T15:24:47.490736Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# create data generator to get data in batch (avoids session crash)\ndef data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n    # loop over images\n    X1, X2, y = list(), list(), list()\n    n = 0\n    while 1:\n        for key in data_keys:\n            n += 1\n            captions = mapping[key]\n            # process each caption\n            for caption in captions:\n                # encode the sequence\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                # split the sequence into X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pairs\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    \n                    # store the sequences\n                    X1.append(features[key][0])\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                yield [X1, X2], y\n                X1, X2, y = list(), list(), list()\n                n = 0","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:47.493887Z","iopub.execute_input":"2023-07-30T15:24:47.494441Z","iopub.status.idle":"2023-07-30T15:24:47.508121Z","shell.execute_reply.started":"2023-07-30T15:24:47.494350Z","shell.execute_reply":"2023-07-30T15:24:47.506592Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Model Creation","metadata":{}},{"cell_type":"code","source":"# train the model\nepochs = 20\nbatch_size = 32\nsteps = len(train) // batch_size\n\nfor i in range(epochs):\n    # create data generator\n    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n    # fit for one epoch\n    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-30T15:24:49.442579Z","iopub.execute_input":"2023-07-30T15:24:49.442966Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"227/227 [==============================] - 76s 316ms/step - loss: 5.2229\n227/227 [==============================] - 71s 314ms/step - loss: 4.0357\n227/227 [==============================] - 68s 297ms/step - loss: 3.6133\n227/227 [==============================] - 74s 326ms/step - loss: 3.3455\n227/227 [==============================] - 67s 294ms/step - loss: 3.1458\n227/227 [==============================] - 72s 316ms/step - loss: 2.9971\n227/227 [==============================] - 71s 314ms/step - loss: 2.8867\n227/227 [==============================] - 70s 307ms/step - loss: 2.7974\n227/227 [==============================] - 71s 312ms/step - loss: 2.7146\n227/227 [==============================] - 71s 311ms/step - loss: 2.6421\n227/227 [==============================] - 71s 312ms/step - loss: 2.5790\n227/227 [==============================] - 72s 317ms/step - loss: 2.5277\n227/227 [==============================] - 72s 315ms/step - loss: 2.4726\n227/227 [==============================] - 69s 304ms/step - loss: 2.4247\n227/227 [==============================] - 70s 310ms/step - loss: 2.3805\n227/227 [==============================] - 71s 313ms/step - loss: 2.3421\n227/227 [==============================] - 72s 317ms/step - loss: 2.3095\n227/227 [==============================] - 72s 316ms/step - loss: 2.2687\n203/227 [=========================>....] - ETA: 7s - loss: 2.2303","output_type":"stream"}]},{"cell_type":"code","source":"# save the model\nmodel.save(WORKING_DIR+'/best_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Captions for the Image","metadata":{}},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate caption for an image\ndef predict_caption(model, image, tokenizer, max_length):\n    # add start tag for generation process\n    in_text = 'startseq'\n    # iterate over the max length of sequence\n    for i in range(max_length):\n        # encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad the sequence\n        sequence = pad_sequences([sequence], max_length)\n        # predict next word\n        yhat = model.predict([image, sequence], verbose=0)\n        # get index with high probability\n        yhat = np.argmax(yhat)\n        # convert index to word\n        word = idx_to_word(yhat, tokenizer)\n        # stop if word not found\n        if word is None:\n            break\n        # append word as input for generating next word\n        in_text += \" \" + word\n        # stop if we reach end tag\n        if word == 'endseq':\n            break\n      \n    return in_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n# validate with test data\nactual, predicted = list(), list()\n\nfor key in tqdm(test):\n    # get actual caption\n    captions = mapping[key]\n    # predict the caption for image\n    y_pred = predict_caption(model, features[key], tokenizer, max_length) \n    # split into words\n    actual_captions = [caption.split() for caption in captions]\n    y_pred = y_pred.split()\n    # append to the list\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n    \n# calcuate BLEU score\nprint(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\nprint(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\ndef generate_caption(image_name):\n    # load the image\n    # image_name = \"1001773457_577c3a7d70.jpg\"\n    image_id = image_name.split('.')[0]\n    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n    image = Image.open(img_path)\n    captions = mapping[image_id]\n    print('---------------------Actual---------------------')\n    for caption in captions:\n        print(caption)\n    # predict the caption\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n    print('--------------------Predicted--------------------')\n    print(y_pred)\n    plt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"1001773457_577c3a7d70.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"107582366_d86f2d3347.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"1087539207_9f77ab3aaf.jpg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}